{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_Sentiment_Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsELyx16UhjQ",
        "colab_type": "text"
      },
      "source": [
        "Implement Sentiment Analysis on Twitter Airline Review Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNNnqiksfaps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "url='https://raw.githubusercontent.com/AKing1998/NLP_Assignment_Sentiment_Analysis/master/Tweets.csv'\n",
        "data=pd.read_csv(url)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLr3NchQg1N5",
        "colab_type": "code",
        "outputId": "38416977-d808-4a6b-87ba-2f0d60d6c4e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "\n",
        "import re\n",
        "import nltk.classify.util\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf891fyDnx5v",
        "colab_type": "text"
      },
      "source": [
        "Removing all instances of @usernames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXJ8Sqaomx9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0,14639):\n",
        "  temp=str(data.iloc[i][10])\n",
        "  data.at[i,'text']=re.sub('@[^\\s]+','',temp)\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqV9Owrk98aZ",
        "colab_type": "text"
      },
      "source": [
        "14639 rows Ã— 15 columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd1kfvUdUZUM",
        "colab_type": "text"
      },
      "source": [
        "Removing stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F_rsJ4eZGAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_word_features(words):\n",
        "    useful_words = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "    my_dict = dict([(word, True) for word in useful_words])\n",
        "    return my_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OREYle3KEaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0,14639):\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "  word_tokens = word_tokenize(data.iloc[i][10]) \n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "  filtered_sentence = [] \n",
        "  for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w) \n",
        "  data.at[i,'text']=filtered_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfbNNtZyXeW6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4bdfbe25-e028-4a50-fd6d-ba86d3043bdf"
      },
      "source": [
        "type(data.iloc[123][10])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOl3KhiFYBZV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dfa78d86-a418-41fb-b901-3f7e54d395bc"
      },
      "source": [
        "create_word_features(['hello','hey'])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hello': True, 'hey': True}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP3zaTbb8g_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_reviews=[]\n",
        "negative_reviews=[]\n",
        "neutral_reviews=[]\n",
        "for i in range(0,14639):\n",
        "  if data.iloc[i][1]=='positive':\n",
        "    positive_reviews.append((create_word_features(data.iloc[i][10]), \"positive\"))\n",
        "  elif data.iloc[i][1]=='neutral':\n",
        "    neutral_reviews.append((create_word_features(data.iloc[i][10]), \"negative\"))\n",
        "  elif data.iloc[i][1]=='negative':\n",
        "    negative_reviews.append((create_word_features(data.iloc[i][10]), \"neutral\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdNjaV__B2bY",
        "colab_type": "text"
      },
      "source": [
        "Print the number of positive negative and neutral numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEPbvxgC91Lk",
        "colab_type": "code",
        "outputId": "3e01bed1-e35a-455d-f6f6-00f12fc3f421",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(positive_reviews),len(negative_reviews),len(neutral_reviews))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2363 9178 3098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSIXpIGVjxe_",
        "colab_type": "text"
      },
      "source": [
        "Creating testing and training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFsVdziejw8H",
        "colab_type": "code",
        "outputId": "262b7e37-295d-486f-8d97-506870e3860b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_set=positive_reviews[:1772]+negative_reviews[:6883]+neutral_reviews[:2323]\n",
        "test_set=positive_reviews[1772:]+negative_reviews[6883:]+neutral_reviews[2323:]\n",
        "print(len(train_set),  len(test_set))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10978 3661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sglINRKuhFYg",
        "colab_type": "text"
      },
      "source": [
        "Training the Classifier and testing it with the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoEdagQrowNu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0c20b569-1c3d-42e5-c9cc-23eef3cee98d"
      },
      "source": [
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "accuracy = nltk.classify.util.accuracy(classifier, test_set)\n",
        "print(accuracy * 100)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74.43321496858782\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}